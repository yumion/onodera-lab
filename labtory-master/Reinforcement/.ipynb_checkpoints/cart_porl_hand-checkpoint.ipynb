{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# パッケージのimport# パッケージ \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 定数の設定\n",
    "NUM_DIZITIZED = 6  # 各状態の離散値への分割数\n",
    "GAMMA = 0.99  # 時間割引率\n",
    "ETA = 0.5  # 学習係数\n",
    "MAX_STEPS = 200  # 1試行のstep数\n",
    "NUM_EPISODES = 1000  # 最大試行回数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    '''CartPoleのエージェントクラスです、棒付き台車そのものになります'''\n",
    "\n",
    "    def __init__(self, num_states, num_actions):\n",
    "        self.brain = Brain(num_states, num_actions)  # エージェントが行動を決定するための頭脳を生成\n",
    "\n",
    "    def update_Q_function(self, observation, action, reward, observation_next):\n",
    "        '''Q関数の更新'''\n",
    "        self.brain.update_Q_table(\n",
    "            observation, action, reward, observation_next)\n",
    "\n",
    "    def get_action(self, observation, step):\n",
    "        '''行動の決定'''\n",
    "        action = self.brain.decide_action(observation, step)\n",
    "        return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Brain:\n",
    "    '''エージェントが持つ脳となるクラスです、Q学習を実行します'''\n",
    "\n",
    "    def __init__(self, num_states, num_actions):\n",
    "        self.num_actions = num_actions  # CartPoleの行動（右に左に押す）の2を取得\n",
    "\n",
    "        # Qテーブルを作成。行数は状態を分割数^（4変数）にデジタル変換した値、列数は行動数を示す\n",
    "        self.q_table = np.random.uniform(low=0, high=1, size=(\n",
    "            NUM_DIZITIZED**num_states, num_actions))\n",
    "\n",
    "\n",
    "    def bins(self, clip_min, clip_max, num):\n",
    "        '''観測した状態（連続値）を離散値にデジタル変換する閾値を求める'''\n",
    "        return np.linspace(clip_min, clip_max, num + 1)[1:-1]\n",
    "\n",
    "    def digitize_state(self, observation):\n",
    "        '''観測したobservation状態を、離散値に変換する'''\n",
    "        cart_pos, cart_v, pole_angle, pole_v = observation\n",
    "        digitized = [\n",
    "            np.digitize(cart_pos, bins=self.bins(-2.4, 2.4, NUM_DIZITIZED)),\n",
    "            np.digitize(cart_v, bins=self.bins(-3.0, 3.0, NUM_DIZITIZED)),\n",
    "            np.digitize(pole_angle, bins=self.bins(-0.5, 0.5, NUM_DIZITIZED)),\n",
    "            np.digitize(pole_v, bins=self.bins(-2.0, 2.0, NUM_DIZITIZED))\n",
    "        ]\n",
    "        return sum([x * (NUM_DIZITIZED**i) for i, x in enumerate(digitized)])\n",
    "\n",
    "    def update_Q_table(self, observation, action, reward, observation_next):\n",
    "        '''QテーブルをQ学習により更新'''\n",
    "        state = self.digitize_state(observation)  # 状態を離散化\n",
    "        state_next = self.digitize_state(observation_next)  # 次の状態を離散化\n",
    "        Max_Q_next = max(self.q_table[state_next][:])\n",
    "        self.q_table[state, action] = self.q_table[state, action] + \\\n",
    "            ETA * (reward + GAMMA * Max_Q_next - self.q_table[state, action])\n",
    "\n",
    "    def decide_action(self, observation, episode):\n",
    "        '''ε-greedy法で徐々に最適行動のみを採用する'''\n",
    "        state = self.digitize_state(observation)\n",
    "        epsilon = 0.5 * (1 / (episode + 1))\n",
    "\n",
    "        if epsilon <= np.random.uniform(0, 1):\n",
    "            action = np.argmax(self.q_table[state][:])\n",
    "        else:\n",
    "            action = np.random.choice(self.num_actions)  # 0,1の行動をランダムに返す\n",
    "        return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class  EnvironmentEnviron :\n",
    "\n",
    "    #####################  ロボットハンドのためのメソッド  #############################\n",
    "\n",
    "    def __init__(self):\n",
    "        num_states = 4  # 課題の状態の数(面積と重心と、それぞれの変化量)\n",
    "        num_actions = 4  # CartPoleの行動（上下左右に動く）\n",
    "        self.agent = Agent(num_states, num_actions)  # 環境内で行動するAgentを生成\n",
    "        self.video = cv2.VideoCapture(0) # カメラ起動\n",
    "        \n",
    "        init_frame = self.video.read()\n",
    "        total_area =  np.prod(init_frame.shape[:1])\n",
    "        self.area_threth =total_area*0.3 # 画面の3割が赤色になれば終了にする\n",
    "        \n",
    "    def reset(self):\n",
    "        '''環境を初期化する'''\n",
    "        # TODO: ロボットハンドがランダムに進路をとる\n",
    "        frame = self.vidoe.read()\n",
    "        ''' TODO: 赤色の面積, 重心の位置を取得する\n",
    "        area_sum = \n",
    "        area_v = 0\n",
    "        center_pos = \n",
    "        center_v = 0\n",
    "        '''\n",
    "        observation = (area_sum, area_v, center_pos, center_v )\n",
    "        return observation, frame\n",
    "    \n",
    "    def get_env(self, area_sum_before, center_pos_before):\n",
    "        '''環境を認識する'''\n",
    "        # カメラで写真をとりOpeCVで面積と重心を取得する\n",
    "        frame = self.video.read()\n",
    "        '''TODO: 赤色の面積とその変化量, 重心の位置とその変化量を取得する\n",
    "        area_sum = \n",
    "        area_v = area_sum - area_sum_before\n",
    "        center_pos = \n",
    "        center_v = center_pos - center_pos_before\n",
    "        '''\n",
    "        observation = (area_sum, area_v, center_pos, center_v )\n",
    "        return observation, frame\n",
    "    \n",
    "    def act_env(self, observation, action):\n",
    "        '''TODO: 決定したactionに従って、ロボットハンドを動かす'''\n",
    "        if action == 0:\n",
    "            # 前\n",
    "        elif action == 1:\n",
    "            # 後\n",
    "        elif action == 2:\n",
    "            # 右\n",
    "        elif action == 3:\n",
    "            # 左\n",
    "        area_sum, _, center_pos, _ = observation\n",
    "        observation_next, _ = self.get_env(area_sum, center_pos)\n",
    "        done = self.is_done()\n",
    "        return observation_next, done\n",
    "        \n",
    "    def is_done(self, observation):\n",
    "        '''observationによって終了判定をする'''\n",
    "        # TODO: 終了判定は面積以外にある？\n",
    "        done = False\n",
    "        area_sum, area_v, center_pos, center_v = observation\n",
    "        if area_sum > self.area_threth:\n",
    "            done = True\n",
    "        return done\n",
    "    \n",
    "    #########################################################\n",
    "\n",
    "    def run(self):\n",
    "        '''実行'''\n",
    "        complete_episodes = 0  # 195step以上連続で立ち続けた試行数\n",
    "        is_episode_final = False  # 最終試行フラグ\n",
    "        frames = []  # 動画用に画像を格納する変数\n",
    "\n",
    "        for episode in range(NUM_EPISODES):  # 試行数分繰り返す\n",
    "            observation, frame = self.reset()  # 環境の初期化\n",
    "\n",
    "            for step in range(MAX_STEPS):  # 1エピソードのループ\n",
    "\n",
    "                if is_episode_final is True:  # 最終試行ではframesに各時刻の画像を追加していく\n",
    "                    frames.append(frame)\n",
    "\n",
    "                # 行動を求める\n",
    "                action = self.agent.get_action(observation, episode)\n",
    "\n",
    "                # 行動a_tの実行により、s_{t+1}, r_{t+1}を求める\n",
    "                observation_next, done = self.act_env(observation, action)\n",
    "\n",
    "                # 報酬を与える\n",
    "                if done:  # ステップ数が200経過するか、一定角度以上傾くとdoneはtrueになる\n",
    "                    if step < 195:\n",
    "                        reward = -1  # 途中でこけたら罰則として報酬-1を与える\n",
    "                        complete_episodes = 0  # 195step以上連続で立ち続けた試行数をリセット\n",
    "                    else:\n",
    "                        reward = 1  # 立ったまま終了時は報酬1を与える\n",
    "                        complete_episodes += 1  # 連続記録を更新\n",
    "                else:\n",
    "                    reward = 0  # 途中の報酬は0\n",
    "\n",
    "                # step+1の状態observation_nextを用いて,Q関数を更新する\n",
    "                self.agent.update_Q_function(\n",
    "                    observation, action, reward, observation_next)\n",
    "\n",
    "                # 観測の更新\n",
    "                observation = observation_next\n",
    "\n",
    "                # 終了時の処理\n",
    "                if done:\n",
    "                    print('{0} Episode: Finished after {1} time steps'.format(\n",
    "                        episode, step + 1))\n",
    "                    break\n",
    "\n",
    "            if is_episode_final is True:  # 最終試行では動画を保存と描画\n",
    "                display_frames_as_gif(frames)\n",
    "                break\n",
    "\n",
    "            if complete_episodes >= 10:  # 10連続成功なら\n",
    "                print('10回連続成功')\n",
    "                is_episode_final = True  # 次の試行を描画を行う最終試行とする"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
